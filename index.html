<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Contextual Bias</title>
    <link crossorigin="anonymous" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
          integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" rel="stylesheet">
    <script crossorigin="anonymous"
            integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
            src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script>
    <script crossorigin="anonymous"
            integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
            src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
    <script crossorigin="anonymous"
            integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
            src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>
</head>


<!-- <style>
    halign{
		margin-left: auto;
		margin-right: auto;
	}
</style> -->

<body>

<!-- h1 {
    display: inline;
} -->
<br>
<div class="container">

    <center>
    <!-- <img style="display: inline;" src="images/iitd_logo.png" width="100" height="100" align="left"> -->

    <h2>
        Does Data Repair Lead to Fair Models? 
    </h2>
    <h2>
        Curating Contextually Fair Data To Reduce Model Bias
    </h2>

    <!-- <img style="display: inline;" src="images/iiitd_logo.jpg" width="100" height="100" align="right"> -->

    <h5 style="color: #9A2617">Sharat Agarwal<sup>1*</sup>&ensp;   Sumanyu Muku<sup>2*</sup>&ensp;    Chetan Arora<sup>2</sup>&ensp;    Saket Anand<sup>1</sup></h5>
        <div style="color: gray"><sup>1</sup><a href="https://www.iiitd.ac.in/">IIIT Delhi</a>, <sup>2</sup><a href="https://home.iitd.ac.in/">IIT Delhi</a>, *equal contribution</div>
        <div><a href="dummy/paper.html" style="color:#FF0000;">[Paper]</a> <a href="https://github.com/sumanyumuku98/contextual-bias" style="color:#FF0000;">[Code]</a></div>
        <br>
        <h6>
        In IEEE Winter Conference on Applications of Computer Vision (<a href="https://wacv2022.thecvf.com/home">WACV</a>), 2022

    </h6>

    </center>


    <!-- <hr>
            <div style="text-align: center">
                <div class="thumbnail">
                <div class="embed-responsive embed-responsive-4by3" style="width: 50%; left: 27%">
                    <iframe class="embed-responsive-item" src="demo_video.3gp"></iframe>
                </div>
                    <div class="caption" style="color: #6d757d">Task example</div>
                </div>
            </div>
            <br><br>
    <hr> -->
    <hr>
    <center>
    <img src="images/teaser.png" width="60%">
    </center>
    <hr>
    <h4>Abstract</h4>
    Contextual information is a valuable cue for Deep Neural Networks (DNNs) to learn better representations and improve accuracy. 
    However, co-occurrence bias in the training dataset may hamper a DNN model's generalizability to unseen scenarios in the real world. For example, in COCO, many object categories have a much higher co-occurrence with men compared to women, which can bias a DNN's prediction in favor of men. 
    Recent works have focused on task-specific training strategies to handle bias in such scenarios, but fixing the available data is often ignored. In this paper, we propose a novel and more generic solution to address the contextual bias in the datasets by selecting a subset of the samples, 
    which is fair in terms of the co-occurrence with various classes for a protected attribute. 
    We introduce a data repair algorithm using the coefficient of variation, which can curate fair and contextually balanced data for a protected class(es). 
    This helps in training a fair model irrespective of the task, architecture or training methodology. Our proposed solution is simple, effective and can even be used in an active learning setting where the data labels are not present or being generated incrementally. 
    We demonstrate the effectiveness of our algorithm for the task of object detection and multi-label image classification across different datasets. 
    Through a series of experiments, we validate that curating contextually fair data helps make model predictions fair by balancing the true positive rate for the protected class across groups without compromising on the model's overall performance. 
    <hr>
            <h4>Plots/Figures</h4>
            <br>


            <table class="table table-striped table-bordered">
                <caption style="text-align: center">COCO Object Co-occurence for Male Vs Female class.</caption>
                <tr>
                    <td>
                        <img align="center"
                             src="images/gender_stats.png"
                             width="100%">
                    </td>
                </tr>
                <tr>
                    <td width="60%">
                        <code>COCO</code> suffers from gender bias with its 79 object categories. We can see that males are biased in almost all the
                        categories by a significant margin as compared to females, thus this contextual bias is important to be fixed for real world application.               
                    </td>
                </tr>
                
            </table>

            <br>
            <br>

            <table class="table table-striped table-bordered">
                <caption style="text-align: center"> Active Learning Results
                </caption>

                <tr>
                    <td>
                        <img align="center"
                             src="images/ALOFT_2.png"
                             width="100%">
                    </td>
                </tr>
                <tr>
                    <td width="100%">
                        Comparison of ALOFT with other AL baselines. <code>(a)</code> Coefficient of variation for each selection, 
                        lower the value fairer the selection 
                        <code>(b)</code> Representational bias of the model. 
                    </td>
                </tr>

                <tr>
                    <td>
                        <img align="center"
                             src="images/CelebA_AL.png"
                             width="100%">
                    </td>
                
                </tr>
                <tr>
                    <td width="100%">
                        Average precision of detecting <code>(a)</code> Male, <code>(b)</code> Female face in the presence of 39 different attributes, at different budgets in CelebA dataset. 
                        AP of female has significantly improved compromising a bit in Male because of fair sampling.
                    </td>
                </tr>


            </table>

            <!-- <br>
            <br> -->

            <!-- <table class="table table-striped table-bordered">
                <caption style="text-align: center">Other Dataset Statistics
                </caption>

                <tr>
                    <td width="33.33%">
                        <img align="center"
                             src="https://user-images.githubusercontent.com/48205355/53884884-88e15a00-4042-11e9-99a5-8fdd7a46ce68.png"
                             width="100%">
                    </td>
                    <td width="33.33%">
                        <img align="center"
                             src="https://user-images.githubusercontent.com/48205355/53952853-f13e4300-40f7-11e9-9444-7d9a030dc4ae.png"
                             width="100%">
                    </td>
                    <td width="33.33%">
                        <img src="https://user-images.githubusercontent.com/48205355/53885089-ea092d80-4042-11e9-94c3-b4690723cb32.png"
                             width="100%">
                    </td>
                </tr>
                <tr>
                    <td width="33.33%">The figure showing average time per turn in a Dialog, across the Dataset.</td>
                    <td width="33.33%">The figure showing average dialog time, across the Dataset.</td>
                    <td width="33.33%">The figure showing contribution of each speaker in generating humor, across the
                        Dataset.
                    </td>
                </tr>
            </table> -->

            <!--    #### Proposed Model, Multimodal Self Attention Model(MSAM) for Multimodal Humor:-->
            <!--    <table>-->
            <!--        <tr>-->
            <!--            <td width="100%">-->
            <!--                <img src="https://user-images.githubusercontent.com/21227893/58746488-53a5a600-8491-11e9-9769-f1f9093db75e.png">-->
            <!--            </td>-->
            <!--        </tr>-->

            <!--    </table>-->
            <!-- <br><br> -->
            <!-- <h4> MSAM model </h4>
            <table class="table table-striped table-bordered">
                <caption>The figure describing the proposed Multimodal Self Attention Model (MSAM) for the laughter detection task. We obtain features of each joint dialogue turn using Multimodal Self attention network. We then obtain the final feature vector using a sequential network before feeding the resultant vector to the binary classifier.</caption>
                <tr>
                    <td width="60%">
                        <img align="center" src="assets/m.png" width="100%">
                    </td>
                </tr>
            </table>

            <br><br> -->
            <hr>

            <h4> Qualitative results </h4>
            <table class="table table-striped table-bordered">
                <caption>
                    Detection of <code>CUP</code>  class by different Active Learning techniques on Dollar Street and ObjectNet dataset. 
                    Green box denotes predicted bounding box of <code>CUP</code> when model was trained using only <code>20%</code> of training data.
                </caption>
                <tr>
                    <td>
                        <img align="center" src="images/dollar_street_cup.png" width="100%">
                    </td>
                </tr>
            </table>

            <br><br>
            <!-- <h4> Explaining humor </h4>
            <table class="table table-striped table-bordered">
                <caption>
                    The left column shows visualization of attention at the word level and the right column shows attention visualization at turn level.
                </caption>
                <tr>
                    <td>
                        <img align="center" src="assets/c.png" width="100%">
                    </td>
                </tr>
            </table>

            <br><br>
            <h4> Baseline Models </h4>
            <table class="table table-striped table-bordered">
                <caption style="text-align: center">Fusion Models</caption>

                <tr>
                    <td width="50%">
                        <img align="center"
                             src="https://user-images.githubusercontent.com/48205355/53886442-0e1a3e00-4046-11e9-87a3-259d68593d62.png"
                             width="100%">
                    </td>
                    <td width="50%">
                        <img align="center"
                             src="https://user-images.githubusercontent.com/48205355/53886443-0e1a3e00-4046-11e9-808b-6e50ec5a9b04.png"
                             width="100%">
                    </td>
                </tr>

                <tr>
                    <td align="center" width="50%">Text based Fusion Model (TFM)</td>
                    <td align="center" width="50%">Video based Fusion Model (VFM)</td>
                </tr>
            </table>


            <table class="table table-striped table-bordered">
                <caption style="text-align: center">Attention Models</caption>

                <tr>
                    <td width="50%">
                        <img align="center"
                             src="https://user-images.githubusercontent.com/48205355/53887017-5f76fd00-4047-11e9-97d6-7e690101f2a9.png"
                             width="100%">
                    </td>
                    <td width="50%">
                        <img align="center"
                             src="https://user-images.githubusercontent.com/48205355/53887018-600f9380-4047-11e9-94b9-062446d18306.png"
                             width="100%">
                    </td>
                </tr>

                <tr>
                    <td align="center" width="50%">Text based Attention Model (TAM)</td>
                    <td align="center" width="50%">Video based Attention Model (VAM)</td>
                </tr>
            </table> -->

            <br>
            <br>
            <br>
    <br><br>


</div>
</body>
</html>
